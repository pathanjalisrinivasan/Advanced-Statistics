{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb99f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1\n",
    "#univariate - Didnt do any data cleaning as it looked like its not required. Just showed the plots \n",
    "fig, axes = plt.subplots(nrows=4,ncols=2)\n",
    "fig.set_size_inches(12, 20)\n",
    "\n",
    "a = sns.distplot(ds['Apps'] , ax=axes[0][0])\n",
    "a.set_title(\"Apps Distribution\",fontsize=10)\n",
    "\n",
    "a = sns.boxplot(ds['Apps'] , orient = \"v\" , ax=axes[0][1])\n",
    "a.set_title(\"Apps Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(ds['Accept'] , ax=axes[1][0])\n",
    "a.set_title(\"Accept Distribution\",fontsize=10)\n",
    "fig, axes = plt.subplots(nrows=4,ncols=2)\n",
    "fig.set_size_inches(12, 20)\n",
    "\n",
    "a = sns.distplot(ds['Apps'] , ax=axes[0][0])\n",
    "a.set_title(\"Apps Distribution\",fontsize=10)\n",
    "\n",
    "a = sns.boxplot(ds['Apps'] , orient = \"v\" , ax=axes[0][1])\n",
    "a.set_title(\"Apps Distribution\",fontsize=15)\n",
    "\n",
    "a = sns.distplot(ds['Accept'] , ax=axes[1][0])\n",
    "a.set_title(\"Accept Distribution\",fontsize=10)\n",
    "\n",
    "a = sns.boxplot(ds['Accept'] , orient = \"v\" , ax=axes[1][1])\n",
    "a.set_title(\"Accept Distribution\",fontsize=10)\n",
    "\n",
    "a = sns.distplot(ds['Enroll'] , ax=axes[2][0])\n",
    "a.set_title(\"Enroll Distribution\",fontsize=10)\n",
    "\n",
    "a = sns.boxplot(ds['Enroll'] , orient = \"v\" , ax=axes[2][1])\n",
    "a.set_title(\"Enroll Distribution\",fontsize=10)\n",
    "\n",
    "a = sns.distplot(ds['Top10perc'] , ax=axes[3][0])\n",
    "a.set_title(\"Top10perc Distribution\",fontsize=10)\n",
    "\n",
    "a = sns.boxplot(ds['Top10perc'] , orient = \"v\" , ax=axes[3][1])\n",
    "a.set_title(\"Top10perc Distribution\",fontsize=10)\n",
    "\n",
    "#bivariate\n",
    "sns.pairplot(ds, diag_kind='kde');\n",
    "\n",
    "#2.2 Applying scaling - Yes, necessary\n",
    "z_apply=ds.apply(zscore)\n",
    "z_apply.head()\n",
    "\n",
    "#2.3 covairance & coorelation\n",
    "cov = np.cov(ds.T)\n",
    "corr = ds.corr()\n",
    "\n",
    "#2.4 outliers before and after scaling\n",
    "ds.boxplot()\n",
    "z_apply.boxplot()\n",
    "\n",
    "#2.5 Eigenvectors & values\n",
    "pca = PCA(n_components=17, random_state=123)\n",
    "dspca = pca.fit_transform(ds)\n",
    "pca.components_\n",
    "pca.explained_variance_\n",
    "\n",
    "#2.6\n",
    "comp = pd.DataFrame(pca.components_.T,columns=list(ds))\n",
    "pd.DataFrame(pca.components_.T, columns = ['PC1','PC2', 'PC3', 'PC4', 'PC5', 'PC6','PC7','PC8', 'PC9', 'PC10', 'PC11', 'PC12','PC13','PC14','PC15','PC16','PC17'],\n",
    "                                    index = ds.columns)\n",
    "\n",
    "#2.7\n",
    "0.25 * Apps + 0.21 * Accept + 0.18 * Enroll + 0.35 * Top10perc + 0.34 * Top25perc + 0.15 * F.Undergrad + 0.03 * P.Undergrad + 0.29 * Outstate + 0.25 * Room.Board + 0.06 * Books + -0.04 * Personal + 0.32 * PhD + 0.32 * Terminal + -0.18 * S.F.Ratio + 0.21 * perc.alumni + 0.32 * Expend + 0.25 * Grad.Rate\n",
    "\n",
    "#2.8 - after showing the scree plot explained that we would need somewhere around 7-8 PCA as it covers around 85%data\n",
    "sns.lineplot(y=pca.explained_variance_ratio_ ,x=range(1,18),marker='o')\n",
    "plt.xlabel('Number of Components',fontsize=10)\n",
    "plt.ylabel('Variance Explained',fontsize=10)\n",
    "plt.title('Scree Plot',fontsize=12)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "#2.9 - Explained PCA steps and something about PCA\n",
    "Initially there were 17 cloumns of data, as it stands its a tedious process to analyse all the 17 columns in the dataframe and to arrive at conclusion.\n",
    "By the help of Principal Component Analysis, using the correlation matirix we can find out the which components tend to have higher correlation with each other followed by finding out the eigenvalues & eigenvectors.\n",
    "PCA works best on data set having 3 or higher dimensions. Because, with higher dimensions, it becomes increasingly difficult to make interpretations from the resultant cloud of data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
